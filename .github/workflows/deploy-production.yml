# OpsSight DevOps Platform - Production Deployment Workflow
# Secure production deployment with manual approvals following cursor development standards

name: ðŸŒŸ Deploy to Production

on:
  push:
    tags: ['v*']
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Docker image tag to deploy (must be a version tag)'
        required: true
      skip_backup:
        description: 'Skip database backup'
        required: false
        default: 'false'
        type: boolean
      maintenance_mode:
        description: 'Enable maintenance mode during deployment'
        required: false
        default: 'true'
        type: boolean

env:
  # Reason: Production environment configuration
  ENVIRONMENT: production
  CLUSTER_NAME: opsight-production-cluster
  REGISTRY: ${{ secrets.ECR_REGISTRY || '123456789012.dkr.ecr.us-west-2.amazonaws.com' }}
  FRONTEND_IMAGE: opsight/frontend
  BACKEND_IMAGE: opsight/backend

jobs:
  # Pre-production validation
  pre-production-validation:
    name: ðŸ” Pre-production Validation
    runs-on: ubuntu-latest
    environment: production-validation
    
    outputs:
      image-tag: ${{ steps.vars.outputs.image-tag }}
      deploy-approved: ${{ steps.approval.outputs.approved }}
      
    steps:
      - name: ðŸ“¦ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ·ï¸ Validate and set deployment variables
        id: vars
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            IMAGE_TAG="${{ github.event.inputs.image_tag }}"
          else
            IMAGE_TAG="${{ github.ref_name }}"
          fi
          
          # Reason: Ensure production deployments use semantic versioning
          if [[ ! "$IMAGE_TAG" =~ ^v[0-9]+\.[0-9]+\.[0-9]+(-[a-zA-Z0-9]+)?$ ]]; then
            echo "âŒ Invalid version tag: $IMAGE_TAG"
            echo "Production deployments require semantic version tags (e.g., v1.0.0)"
            exit 1
          fi
          
          echo "image-tag=${IMAGE_TAG}" >> $GITHUB_OUTPUT
          echo "ðŸ“‹ Validated deployment tag: ${IMAGE_TAG}"

      - name: ðŸ” Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: github-actions-prod-validation
          aws-region: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: âœ… Verify production images exist
        run: |
          aws ecr describe-images \
            --repository-name opsight/frontend \
            --image-ids imageTag=${{ steps.vars.outputs.image-tag }} \
            --region ${{ secrets.AWS_REGION || 'us-west-2' }} || {
            echo "âŒ Frontend production image not found"
            exit 1
          }
          
          aws ecr describe-images \
            --repository-name opsight/backend \
            --image-ids imageTag=${{ steps.vars.outputs.image-tag }} \
            --region ${{ secrets.AWS_REGION || 'us-west-2' }} || {
            echo "âŒ Backend production image not found"
            exit 1
          }
          
          echo "âœ… All production images verified"

      - name: ðŸ” Security scan verification
        run: |
          # Reason: Ensure images have passed security scans
          echo "ðŸ” Verifying security scan results for production deployment..."
          
          # TODO: Add actual security scan verification logic here
          # This would typically check scan results from ECR or security tools
          
          echo "âœ… Security scan verification completed"

      - name: âœ‹ Manual approval checkpoint
        id: approval
        uses: actions/github-script@v6
        with:
          script: |
            const { data: deployment } = await github.rest.repos.createDeployment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              ref: context.sha,
              environment: 'production',
              description: `Production deployment of ${{ steps.vars.outputs.image-tag }}`,
              required_contexts: []
            });
            
            core.setOutput('approved', 'true');
            return deployment.id;

  # Database backup
  database-backup:
    name: ðŸ—ƒï¸ Database Backup
    runs-on: ubuntu-latest
    needs: pre-production-validation
    if: github.event.inputs.skip_backup != 'true'
    environment: production
    
    steps:
      - name: ðŸ” Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: github-actions-db-backup
          aws-region: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: âš™ï¸ Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: ðŸ”— Configure kubectl
        run: |
          aws eks update-kubeconfig \
            --name ${{ env.CLUSTER_NAME }} \
            --region ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: ðŸ—ƒï¸ Create database backup
        run: |
          BACKUP_NAME="pre-deployment-$(date +%Y%m%d-%H%M%S)-${{ needs.pre-production-validation.outputs.image-tag }}"
          
          kubectl create job "${BACKUP_NAME}" \
            --from=cronjob/db-backup \
            --namespace=production
          
          # Reason: Wait for backup to complete
          kubectl wait --for=condition=complete \
            --timeout=600s \
            job/"${BACKUP_NAME}" \
            --namespace=production
          
          # Reason: Verify backup success
          if ! kubectl get job "${BACKUP_NAME}" -o jsonpath='{.status.succeeded}' --namespace=production | grep -q 1; then
            echo "âŒ Database backup failed"
            kubectl logs job/"${BACKUP_NAME}" --namespace=production
            exit 1
          fi
          
          echo "âœ… Database backup completed: ${BACKUP_NAME}"

  # Maintenance mode
  enable-maintenance:
    name: ðŸš§ Enable Maintenance Mode
    runs-on: ubuntu-latest
    needs: [pre-production-validation, database-backup]
    if: always() && github.event.inputs.maintenance_mode != 'false' && (needs.database-backup.result == 'success' || needs.database-backup.result == 'skipped')
    environment: production
    
    steps:
      - name: ðŸ” Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: github-actions-maintenance
          aws-region: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: âš™ï¸ Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: ðŸ”— Configure kubectl
        run: |
          aws eks update-kubeconfig \
            --name ${{ env.CLUSTER_NAME }} \
            --region ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: ðŸš§ Enable maintenance mode
        run: |
          # Reason: Scale down frontend to show maintenance page
          kubectl scale deployment maintenance-page \
            --replicas=2 \
            --namespace=production
          
          # Reason: Wait for maintenance page to be ready
          kubectl rollout status deployment/maintenance-page \
            --namespace=production \
            --timeout=120s
          
          echo "âœ… Maintenance mode enabled"

  # Production deployment
  deploy-production:
    name: ðŸŒŸ Deploy to Production
    runs-on: ubuntu-latest
    needs: [pre-production-validation, database-backup, enable-maintenance]
    if: always() && needs.pre-production-validation.outputs.deploy-approved == 'true' && (needs.enable-maintenance.result == 'success' || needs.enable-maintenance.result == 'skipped')
    environment: production
    
    strategy:
      matrix:
        component: [backend, frontend]
        
    steps:
      - name: ðŸ“¦ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ” Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: github-actions-prod-deploy
          aws-region: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: âš™ï¸ Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: ðŸ”— Configure kubectl
        run: |
          aws eks update-kubeconfig \
            --name ${{ env.CLUSTER_NAME }} \
            --region ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: ðŸŒŸ Deploy ${{ matrix.component }}
        run: |
          IMAGE="${{ env.REGISTRY }}/${{ matrix.component == 'frontend' && env.FRONTEND_IMAGE || env.BACKEND_IMAGE }}"
          DEPLOYMENT="${{ matrix.component }}-deployment"
          
          # Reason: Create deployment backup before update
          kubectl get deployment "${DEPLOYMENT}" \
            --namespace=production \
            -o yaml > "${DEPLOYMENT}-backup.yaml"
          
          # Reason: Update deployment with new image
          kubectl set image deployment/"${DEPLOYMENT}" \
            ${{ matrix.component }}="${IMAGE}:${{ needs.pre-production-validation.outputs.image-tag }}" \
            --namespace=production
          
          # Reason: Add deployment annotations for tracking
          kubectl annotate deployment/"${DEPLOYMENT}" \
            "github.com/sha=${{ github.sha }}" \
            "github.com/run-id=${{ github.run_id }}" \
            "github.com/version=${{ needs.pre-production-validation.outputs.image-tag }}" \
            --namespace=production --overwrite

      - name: â³ Wait for rollout
        run: |
          kubectl rollout status deployment/${{ matrix.component }}-deployment \
            --namespace=production \
            --timeout=600s

      - name: ðŸ§ª Run production health checks
        run: |
          SERVICE_URL=$(kubectl get service ${{ matrix.component }}-service \
            --namespace=production \
            -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          
          if [[ -n "$SERVICE_URL" ]]; then
            # Reason: Wait for load balancer to be ready
            sleep 60
            
            # Reason: Comprehensive health checks
            for i in {1..5}; do
              if curl -f "https://${SERVICE_URL}/health" --max-time 30; then
                echo "âœ… ${{ matrix.component }} health check passed (attempt $i)"
                break
              elif [[ $i -eq 5 ]]; then
                echo "âŒ ${{ matrix.component }} health check failed after 5 attempts"
                exit 1
              else
                echo "âš ï¸ ${{ matrix.component }} health check failed (attempt $i), retrying..."
                sleep 30
              fi
            done
          else
            echo "âš ï¸ ${{ matrix.component }} service endpoint not available for testing"
          fi

  # Disable maintenance mode
  disable-maintenance:
    name: ðŸŸ¢ Disable Maintenance Mode
    runs-on: ubuntu-latest
    needs: [deploy-production, enable-maintenance]
    if: always() && needs.deploy-production.result == 'success'
    environment: production
    
    steps:
      - name: ðŸ” Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          role-session-name: github-actions-maintenance-off
          aws-region: ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: âš™ï¸ Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: ðŸ”— Configure kubectl
        run: |
          aws eks update-kubeconfig \
            --name ${{ env.CLUSTER_NAME }} \
            --region ${{ secrets.AWS_REGION || 'us-west-2' }}

      - name: ðŸŸ¢ Disable maintenance mode
        run: |
          # Reason: Scale down maintenance page
          kubectl scale deployment maintenance-page \
            --replicas=0 \
            --namespace=production
          
          echo "âœ… Maintenance mode disabled"

  # Post-deployment verification
  post-deployment:
    name: âœ… Post-deployment Verification
    runs-on: ubuntu-latest
    needs: [deploy-production, disable-maintenance]
    if: always()
    environment: production
    
    steps:
      - name: ðŸ“Š Generate production deployment report
        run: |
          echo "## ðŸŒŸ Production Deployment Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** Production" >> $GITHUB_STEP_SUMMARY
          echo "**Version:** ${{ needs.pre-production-validation.outputs.image-tag }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Deployer:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### Deployment Status" >> $GITHUB_STEP_SUMMARY
          echo "- Overall: ${{ needs.deploy-production.result == 'success' && 'âœ… Success' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Database Backup: ${{ needs.database-backup.result == 'success' && 'âœ… Success' || needs.database-backup.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
          echo "- Maintenance Mode: ${{ needs.disable-maintenance.result == 'success' && 'âœ… Disabled' || 'âš ï¸ Still Active' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.deploy-production.result }}" == "success" ]]; then
            echo "ðŸŽ‰ **Production deployment completed successfully!**" >> $GITHUB_STEP_SUMMARY
            echo "ðŸŒ **Application is now live with version ${{ needs.pre-production-validation.outputs.image-tag }}**" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Production deployment encountered issues.**" >> $GITHUB_STEP_SUMMARY
            echo "ðŸš¨ **Manual intervention may be required.**" >> $GITHUB_STEP_SUMMARY
          fi 